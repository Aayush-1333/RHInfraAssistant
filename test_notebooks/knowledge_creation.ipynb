{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Creation - Storing docs on ChromaDB instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x7fbbf176ae70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chromadb import HttpClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chroma_client = HttpClient(host=os.environ['HOST'], port=8000)\n",
    "chroma_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents!\n"
     ]
    }
   ],
   "source": [
    "# Now let's load and parse the word files using langchain\n",
    "from langchain_community.document_loaders import Docx2txtLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"./RH_Docs\",\n",
    "    glob=\"**/*\",\n",
    "    loader_kwargs={\n",
    "        \".docx\": Docx2txtLoader\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13\n"
     ]
    }
   ],
   "source": [
    "# Let's chunk these docs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "txt_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \".\"],\n",
    "    chunk_size=8192,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_chunks = txt_splitter.split_documents(docs)\n",
    "print(f\"Total chunks: {len(doc_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee573577-bb36-4331-8d25-ba863ae091de', 'b295135e-70ae-4064-bffc-412daf7bfa8e', '306fbc5f-6851-4063-a3fa-f48122741403', '668dd304-cd2f-4d67-822f-bbe8de85a218', 'ef45a6f6-bd5f-4d89-ba62-2ab0890fda3a', 'e035088f-e149-4eae-8391-650bddca8929', '22cc3995-4911-451f-bb7d-04f65c3f1928', 'bb3308b8-42cc-4e80-8c7f-15450210564f', 'aa2564fb-53ae-40af-b09a-60e2991a6d11', '2fbebd78-a6e2-4b1e-858a-38c04dc1eede', 'adf4f1dd-d5aa-4280-8b43-c3410630b59b', '72d98749-61fe-4448-8832-6b870132f4fd', 'b0942fe9-fbf9-4d5e-98e2-6cd458cf3a2e']\n"
     ]
    }
   ],
   "source": [
    "# Now as we got the chunks, let's load them to vector database \n",
    "# along with the embedding function to embed the chunks to vectors\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL=\"bge-m3:latest\"\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"red_hat\",\n",
    "    embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=f\"http://{os.environ['HOST']}:11434\"),\n",
    "    client=chroma_client\n",
    ")\n",
    "\n",
    "chunk_ids = vector_store.add_documents(doc_chunks)\n",
    "print(chunk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back up your cluster’s etcd data by performing a single invocation of the backup script on a control plane host.\n",
      "\n",
      "Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.\n",
      "\n",
      "Decisions:\n",
      "\n",
      "1.\tCUSTOMER Team has confirmed to take etcd backup and store in NFS Server. Details are added in the Pre-Req sheet.\n",
      "\n",
      "Note: At This point of Time, complete rollback up of the Openshift cluster is not supported. For more information please refer to:\n",
      "\n",
      "https://docs.openshift.com/container-platform/4.14/backup_and_restore/control_plane_backup_an d_restore/backing-up-etcd.html\n",
      "\n",
      "12 DNS requirements(Section 23.1.5):\n",
      "\n",
      "https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/installing/installing-on-an y-platform#installation-user-provisioned-validating-dns_installing-platform-agnostic\n",
      "\n",
      "\n",
      "\n",
      "Software Versions\n",
      "\n",
      "Product Version OpenShift Container Platform 4.14 OpenShift Data Foundation 4.14 Red Hat Enterprise Linux 9.x\n",
      "\n",
      "Hardware Supportability.\n",
      "\n",
      "Server Role Hardware No of Nodes Hardware Supportability Matrix Master HPE ProLiant DL360 Gen10 3 https://catalog.redhat.com/hardware/servers/deta il/6589 Worker HPE ProLiant DL560 Gen10 10 https://catalog.redhat.com/hardware/servers/deta il/6449\n",
      "\n",
      "Red Hat Advanced Cluster Security:\n",
      "\n",
      "During the design discussion, CUSTOMER asked to install RHACS on the cluster which would self manage itself. CUSTOMER also requested to create a gatekeeper policy on RHACS to ensure Selinux is enforced for all RWX persistent Volumes.\n",
      "CUSTOMER \n",
      "\n",
      "Red Hat Openshift Platform 4.14 &\n",
      "\n",
      "Red Hat Ceph Storage\n",
      "\n",
      "Low Level Solution Design Document\n",
      "\n",
      "Purpose\n",
      "\n",
      "This document has been produced to centrally capture the information which has been discussed during design workshop with the CUSTOMER  team w.r.t. deployment of RHOCP & RHCS environments.\n",
      "\n",
      "The document covers low level design details describing an RHOCP 4.14 & RHCS 6 platform design. Additionally, it provides guidelines for solution implementation and validation.\n",
      "\n",
      "Scope\n",
      "\n",
      "The scope of this document is to describe the low level design that Red Hat can offer and support by leveraging the Features provided by Red Hat Openshift Container Platform (RHOCP) & Red Hat Ceph Storage (RHCS).\n",
      "\n",
      "This LLD Solution Design document is the result of the design workshop and should cover all requirements that CUSTOMER  has put forward during the workshop.\n",
      "\n",
      "\n",
      "\n",
      "Out Of Scope\n",
      "\n",
      "This LLD document limits the discussions and recommendations only to Red Hat Components involved for RHOCP & RHCS Clusters. Anything not explicitly listed as in-scope is deemed to be out of scope of this design document.\n",
      "\n",
      "\n",
      "\n",
      "Design Workshop\n",
      "\n",
      "An RHOCP & RHCS architecture & design workshop was conducted for CUSTOMER . This time was utilized to go over the RHOCP & RHCS high level architectural components, Its applications and all other needed details required to deploy the cluster including the open items, challenges & expectations from the Customer.\n",
      "\n",
      "In the workshop session, it’s been decided to go ahead with OCP 4.14 deployment using UPI (User Provisioned Installation) in Connected mode. The workshop scope was confined to the OCP & Ceph design.\n",
      "\n",
      "CUSTOMER  application requirement is to use containerization technology of RHOCP to host their EDA Application.\n",
      "\n",
      "Technical Components & High Level Architecture\n",
      "\n",
      "This section covers the key technical components that will be deployed for CUSTOMER  at its data center in Bogura, Bangladesh.\n",
      "\n",
      "As an introduction to OCP; OpenShift Container Platform is a platform for developing and running containerized applications. It is designed to allow applications and the data centers that support them to expand from just a few machines and applications to thousands of machines that serve millions of clients. With its foundation in Kubernetes, OpenShift Container Platform incorporates the same technology that serves as the engine for massive telecommunications, streaming video, gaming, banking and other applications. Its implementation in open Red Hat technologies lets you extend your containerized applications beyond a single cloud to on-premise and multi-cloud environments. Red Hat OpenShift Platform adds:\n",
      "\n",
      "Source code management, builds, and deployments for developers\n",
      "\n",
      "Managing and promoting images at scale as they flow through your system\n",
      "\n",
      "Application management at scale\n",
      "\n",
      "Team and user tracking for organizing a large developer organization\n",
      "\n",
      "Networking infrastructure that supports the cluster\n",
      "\n",
      "Figure - 3.a - High Level RHOCP Component Level Reference Architecture\n",
      "\n",
      "\n",
      "\n",
      "RHOCP will be powered by Red Hat OpenShift Data Foundation which provides persistent software-defined storage based on Red Hat Ceph Storage and optimized for OpenShift. Here, we will be connecting to an existing Ceph storage cluster which was deployed for the CMS application. A new dedicated pool would be created in Ceph Storage and ODF will consume storage from ceph Storage. ODF will be deployed with external mode.\n",
      "\n",
      "Below diagram depicts architecture concluded for OCP 4.14 at CUSTOMER .\n",
      "\n",
      "Figure - 3.b - Logical Diagram concluded at CUSTOMER \n",
      "\n",
      "\n",
      "\n",
      "Infrastructure Components\n",
      "\n",
      "Kubernetes Infrastructure\n",
      "\n",
      "Within the OpenShift Container Platform, Kubernetes manages containerized applications across a set of containers or hosts and provides mechanisms for deployment, maintenance, and application-scaling. The container runtime packages, instantiates, and runs containerized applications. The control plane, which is composed of master machines, manages the OpenShift Container Platform cluster. The control plane machines manage workloads running on the worker machines. The cluster itself manages all upgrades to the machines by the actions of the Cluster Version Operator, the Machine Config Operator, and a set of individual Operators.\n",
      "\n",
      "Key Components of Openshift :-\n",
      "\n",
      "Kubernetes API Server: The Kubernetes API server validates and configures the data for Pods, Services, and replication controllers. It also provides a focal point for the cluster's shared state.\n",
      "\n",
      "Kube-Scheduler: A scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on.\n",
      "\n",
      "Etcd: etcd stores the persistent master state while other components watch etcd for changes to bring themselves into the specified state. Configmaps and Secrets inside the etcd database will be stored on-disk in an encrypted format.\n",
      "\n",
      "Controller Manager Server: The Controller Manager Server watches etcd for changes to objects such as replication, namespace, and service account controller objects, and then uses the API to enforce the specified state. Several such processes create a cluster with one active leader at a time.\n",
      "\n",
      "The CRI-O container engine (crio), which runs and manages the containers. OpenShift Container Platform 4.14 uses CRI-O instead of the Docker Container Engine.\n",
      "\n",
      "Operators: An Operator is a method of packaging, deploying and managing a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl tooling.\n",
      "\n",
      "Operator Lifecycle Manager (OLM): The Operator Lifecycle Manager helps users install, update, and manage the lifecycle of all Operators and their associated services running across their clusters.\n",
      "\n",
      "\n",
      "\n",
      "Design decisions\n",
      "\n",
      "Red  Hat  OCP  4.14  cluster   to  be  deployed   on BareMetal using the Connected User Provisioned Installation.\n",
      "\n",
      "CUSTOMER  will provide Internet using Internet Proxy required for deployment of OCP, Detail will be added in Pre-req Sheet.\n",
      "\n",
      "CUSTOMER  will use Hardware Load Balancer (F5 Load Balancer) required for LB VIPs in L4/Passthrough mode.\n",
      "\n",
      "New OCP 4.14 cluster name will be pocpeda-nexp.CUSTOMER .com\n",
      "\n",
      "There will be 1 bastion machine with the latest RHEL 9.x Installed. Bastion node will be a virtual machine deployed on RHV.\n",
      "\n",
      "Infra components such as Router, Registry, Loki and Prometheus Monitoring will be enabled on master nodes only.\n",
      "\n",
      "For Logging Loki will be enabled with 1x.medium sizing. Refer article\n",
      "\n",
      "RHCOS based 3 x Bare Metal machines will host the controlplane for RHOCP cluster and maintain the high availability of the master and etcd.\n",
      "\n",
      "Below  mentioned  2  VIP’s1  will  be  provided  by  the  CUSTOMER  team  from  F5  Hardware  Load Balancer for each cluster.\n",
      "\n",
      "API VIP\n",
      "\n",
      "API-INT VIP\n",
      "\n",
      ".apps VIP\n",
      "\n",
      "api and api-int DNS endpoints would point to the same VIP.\n",
      "\n",
      "The CUSTOMER team will provide DNS and NTP servers needed for the OCP environment. It is mandatory to configure forward and reverse lookup in DNS servers for all OCP node’s hostname with its IP address and Load Balancer VIPs aka api and *.apps.\n",
      "\n",
      "CUSTOMER will provide 4 NTP servers.\n",
      "\n",
      "CUSTOMER agreed to use self signed OCP Installer generated (*.apps) wild card certificates to be used for Openshift Ingress Controller.\n",
      "\n",
      "API will use self signed OCP generated certificates.\n",
      "\n",
      "The master API and Web console will be available on port 6443. Please refer to section “5.3 Network Access Requirements” for the complete list of ports that need opening.\n",
      "\n",
      "Other infrastructure requirements are covered in the 5. Implementation Prerequisites.\n",
      "\n",
      "1 VIP:\n",
      "\n",
      "https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/installing/installing-on-bare-metal#ins tallation-load-balancing-user-infra_installing-restricted-networks-bare-metal\n",
      "\n",
      "\n",
      "\n",
      "Separate validation prerequisite spreadsheet has been shared with the CUSTOMER team. All details need to be provided before starting the deployments.\n",
      "\n",
      "CUSTOMER has confirmed to use OVN Kubernetes as a default CNI.\n"
     ]
    }
   ],
   "source": [
    "top_searches = vector_store.similarity_search(query=\"Tell me about RHOCP?\", k=2)\n",
    "\n",
    "for search in top_searches:\n",
    "    print(search.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back up your cluster’s etcd data by performing a single invocation of the backup script on a control plane host.\n",
      "\n",
      "Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.\n",
      "\n",
      "Decisions:\n",
      "\n",
      "1.\tCUSTOMER Team has confirmed to take etcd backup and store in NFS Server. Details are added in the Pre-Req sheet.\n",
      "\n",
      "Note: At This point of Time, complete rollback up of the Openshift cluster is not supported. For more information please refer to:\n",
      "\n",
      "https://docs.openshift.com/container-platform/4.14/backup_and_restore/control_plane_backup_an d_restore/backing-up-etcd.html\n",
      "\n",
      "12 DNS requirements(Section 23.1.5):\n",
      "\n",
      "https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/installing/installing-on-an y-platform#installation-user-provisioned-validating-dns_installing-platform-agnostic\n",
      "\n",
      "\n",
      "\n",
      "Software Versions\n",
      "\n",
      "Product Version OpenShift Container Platform 4.14 OpenShift Data Foundation 4.14 Red Hat Enterprise Linux 9.x\n",
      "\n",
      "Hardware Supportability.\n",
      "\n",
      "Server Role Hardware No of Nodes Hardware Supportability Matrix Master HPE ProLiant DL360 Gen10 3 https://catalog.redhat.com/hardware/servers/deta il/6589 Worker HPE ProLiant DL560 Gen10 10 https://catalog.redhat.com/hardware/servers/deta il/6449\n",
      "\n",
      "Red Hat Advanced Cluster Security:\n",
      "\n",
      "During the design discussion, CUSTOMER asked to install RHACS on the cluster which would self manage itself. CUSTOMER also requested to create a gatekeeper policy on RHACS to ensure Selinux is enforced for all RWX persistent Volumes.\n",
      "CUSTOMER has plans to use multus in future for their workloads, Hence 2 network ports in the server will be left separately for future use of multus. As part of this deployment a second bond will not be configured. It will be configured in LACP bond during day 2 operations in case CUSTOMER has a requirement to add another interface to Pod using multus in future.\n",
      "\n",
      "On all nodes(masters and workers) there will be one LACP bond and 2 nics will be configured under one LACP bond.\n",
      "\n",
      "Openshift Will be integrated with External Ceph Storage using ODF. A dedicated pool would be created on the existing Ceph cluster for EDA application(Pool name would be eda-Customer x).\n",
      "\n",
      "MTU size default 9000 will be used. Effective MTU at pod level will be 8900 as OVN-Kubernetes SDN uses 100 bytes as geneve overhead.\n",
      "\n",
      "The CUSTOMER team will provide a sufficient pool for host networks considering the future growth requirements. This network will include IPs for each OCP node. CUSTOMER Also confirmed maximum Worker Nodes they are planning to add are approx 15.\n",
      "\n",
      "CUSTOMER will provide an L3 network for OCP environment from the same subnet range for all OCP nodes including bastion, bootstrap, master and worker nodes.\n",
      "\n",
      "Users authentication to be configured using local (HTPasswd based authentication provider) as well as LDAP Server. CUSTOMER has provided LDAP Server information in the Pre-Req Sheet.\n",
      "\n",
      "RHOCP will be deployed with only the ipv4 network stack.\n",
      "\n",
      "RHOCP will be integrated with Ceph for all (Block, File and Object) storage requirements.\n",
      "\n",
      "Prometheus, Alert manager, Loki log retention period would be 15 days.\n",
      "\n",
      "Temporary bootstrap VM will be configured on RHV.\n",
      "\n",
      "Red Hat will Install ODF Operator on RHOCP & Integrate it with existing external Red Hat Ceph Storage.\n",
      "\n",
      "A new storage pool would be created for EDA applications in the existing Ceph Storage cluster(eda-Customer ).\n",
      "\n",
      "Storage required for Openshift Registry will be provided from Red Hat Ceph Storage. Rados Gateway will be configured on Ceph for the same. Existing RadosGW ”*.rgw.pocpmp-nexp.CUSTOMER .com” created for the previous\n",
      "\n",
      "\n",
      "\n",
      "cluster would be used for EDA cluster integration.\n",
      "\n",
      "\n",
      "\n",
      "Separate prerequisites sheet has been created and shared with the CUSTOMER team to fill all details pertaining to H/W, network, storage details etc.\n",
      "\n",
      "Please note: Redhat Does not provide any performance guarantee.\n",
      "\n",
      "Container Registry\n",
      "\n",
      "OpenShift Container Platform can utilize any server implementing the container image registry API as a source of images, including the Docker Hub, private registries run by third parties, and the integrated OpenShift Container Platform registry. Below are the options available:\n",
      "\n",
      "Integrated OpenShift Container Platform registry (OCR), OCP internal use, deployed on master nodes.\n",
      "\n",
      "Third Party Registries\n",
      "\n",
      "Red Hat Quay Registries\n",
      "\n",
      "Authentication enabled Red Hat registry\n",
      "\n",
      "Design decisions\n",
      "\n",
      "Registry:\n",
      "\n",
      "Registry (OCR) will be implemented in clusters and will run as a container.\n",
      "\n",
      "The registry stores container images and metadata. If you simply deploy the registry pod, it uses an ephemeral volume that is destroyed if the pod exits. Any images built or pushed into the registry would disappear.\n",
      "\n",
      "Hence, based on the available options at GP, OpenShift Container Platform registry storage will be provided from Red Hat Ceph Storage of size 500GB. CUSTOMER decided to provide us with the Ceph Rados Gateway for Storage. Refer Ceph Design decisions for more details on RGW & Its Load Balancer Requirement.\n",
      "\n",
      "Ceph Storage will be the backend for registry pods & It will be integrated using ODF.\n",
      "\n",
      "\n",
      "\n",
      "Web Console\n",
      "\n",
      "The OpenShift Container Platform web console is a user interface accessible from a web browser. Developers can use the web console to visualize, browse, and manage the contents of projects.\n",
      "\n",
      "Web console will be deployed by default. System administrators may want to access the master apis via a command line tool called oc. Details are covered in the documentation.\n",
      "\n",
      "Design decisions\n",
      "\n",
      "Below decisions are applicable for all clusters.\n",
      "\n",
      "No use-cases were identified. Default web console will be configured.\n",
      "\n",
      "Network Considerations\n",
      "\n",
      "OVN-Kubernetes & DNS\n",
      "\n",
      "The OpenShift Container Platform cluster uses a virtualized network for pod and service networks. The OVN-Kubernetes Container Network Interface (CNI) plug-in is a network provider for the default cluster network. OVN-Kubernetes is based on Open Virtual Network (OVN) and provides an overlay-based networking implementation. A cluster that uses the OVN-Kubernetes network provider also runs Open vSwitch (OVS) on each node. OVN configures OVS on each node to implement the declared network configuration.\n",
      "\n",
      "Design decisions\n",
      "\n",
      "CUSTOMER will use the default OVN-Kubernetes plugin.\n",
      "\n",
      "RHOCP Cluster will be set up with only IPv4 schema.\n",
      "\n",
      "The host network CIDR range for the OCP nodes will be provided by GP.\n",
      "\n",
      "Cluster and Service subnet will be provided by the CUSTOMER team on a prerequisite sheet.\n",
      "\n",
      "All OCP nodes will be reachable to each other. There is no microsegmentation at CUSTOMER Network.\n",
      "\n",
      "\n",
      "\n",
      "The cluster subnet and service subnet will be internal to the OCP cluster. However if applications hosted on this cluster need to connect with some external endpoint, It must not use IP from this cluster subnet and service subnet.\n",
      "\n",
      "2 Interfaces in bond0 will be used for Machine Network. No requirement for additional bond configuration.\n",
      "\n",
      "Ingress Controllers (Routers)\n",
      "\n",
      "The Ingress Operator implements the ingress controller API and is the component responsible for enabling external access to OpenShift Container Platform cluster services. The Operator makes this possible by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources.\n",
      "\n",
      "OCP offers multiple ways in which HAProxy can be used via route objects. Please find the key features/configuration in documentation.\n",
      "\n",
      "Design decisions & Recommendations\n",
      "\n",
      "Below decisions are applicable for the cluster.\n",
      "\n",
      "3 router pods will be deployed on master nodes.\n",
      "\n",
      "RHOCP router is expected to work with HTTP/HTTPS protocol only.\n",
      "\n",
      "By default; router will expose the application over port 443 for https traffic and port 80 for http.\n",
      "\n",
      "The OCP router will be responsible to route the traffic to the pods as per the workflow.\n",
      "\n",
      "Application flow\n",
      "\n",
      "VIP will be configured to pass the traffic to the router pods.\n",
      "\n",
      "Applications can utilize the same VIP and expose the application on intranet. Like https://sample-application.apps.devocp.example.local\n",
      "\n",
      "\n",
      "\n",
      "Load Balancer\n",
      "\n",
      "The CUSTOMER team has confirmed the installation method would be UPI, below are the load balancer requirements.\n",
      "\n",
      "Note:\n",
      "\n",
      "The load balancer must be configured to take a maximum of 30 seconds from the time the API server turns off the /readyz endpoint to the removal of the API server instance from the pool. Within the time frame after /readyz returns an error or becomes healthy, the endpoint must have been removed or added. Probing every 5 or 10 seconds, with two successful requests to become healthy and three to become unhealthy, are well-tested values.\n",
      "\n",
      "API load balancer\n",
      "\n",
      "CUSTOMER would configure F5 Software load balancer for all external API access as below.\n",
      "\n",
      "API endpoint Port VIP ssl mode Backend server Comments api.<cluster_name 6443 passthrough bootstrap bootstrap is >.CUSTOMER .c master 1 temporary and it om master 2 has to be master 3 removed once the cluster is built.\n",
      "\n",
      "Note:\n",
      "\n",
      "Session persistence is not required for the API load balancer to function properly.\n",
      "\n",
      "API-INT load balancer\n",
      "\n",
      "CUSTOMER would configure F5 Software load balancer for all internal API access for the cluster below.\n",
      "\n",
      "API endpoint Port VIP ssl mode Backend server Comments api-int.<cluster_na 22623 passthrough bootstrap bootstrap is me>.Grameenphon and master 1 temporary and e.com 6443 master 2 it has to be master 3 removed once the cluster is\n",
      "\n",
      "built.\n",
      "\n",
      "Note:\n",
      "\n",
      "Session persistence is not required for the API load balancer to function properly.\n",
      "\n",
      ".APPS load balancer\n",
      "\n",
      "CUSTOMER would configure F5 Software load balancer for all application ingress access as below.\n"
     ]
    }
   ],
   "source": [
    "top_searches = vector_store.similarity_search(query=\"Tell me about bastion?\", k=2)\n",
    "\n",
    "for search in top_searches:\n",
    "    print(search.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back up your cluster’s etcd data by performing a single invocation of the backup script on a control plane host.\n",
      "\n",
      "Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.\n",
      "\n",
      "Decisions:\n",
      "\n",
      "1.\tCUSTOMER Team has confirmed to take etcd backup and store in NFS Server. Details are added in the Pre-Req sheet.\n",
      "\n",
      "Note: At This point of Time, complete rollback up of the Openshift cluster is not supported. For more information please refer to:\n",
      "\n",
      "https://docs.openshift.com/container-platform/4.14/backup_and_restore/control_plane_backup_an d_restore/backing-up-etcd.html\n",
      "\n",
      "12 DNS requirements(Section 23.1.5):\n",
      "\n",
      "https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/installing/installing-on-an y-platform#installation-user-provisioned-validating-dns_installing-platform-agnostic\n",
      "\n",
      "\n",
      "\n",
      "Software Versions\n",
      "\n",
      "Product Version OpenShift Container Platform 4.14 OpenShift Data Foundation 4.14 Red Hat Enterprise Linux 9.x\n",
      "\n",
      "Hardware Supportability.\n",
      "\n",
      "Server Role Hardware No of Nodes Hardware Supportability Matrix Master HPE ProLiant DL360 Gen10 3 https://catalog.redhat.com/hardware/servers/deta il/6589 Worker HPE ProLiant DL560 Gen10 10 https://catalog.redhat.com/hardware/servers/deta il/6449\n",
      "\n",
      "Red Hat Advanced Cluster Security:\n",
      "\n",
      "During the design discussion, CUSTOMER asked to install RHACS on the cluster which would self manage itself. CUSTOMER also requested to create a gatekeeper policy on RHACS to ensure Selinux is enforced for all RWX persistent Volumes.\n",
      "OpenShift Container Platform can utilize any server implementing the container image registry API as a source of images, including the Docker Hub, private registries run by third parties, and the integrated OpenShift Container Platform registry. Below are the options available: \n",
      "\n",
      "Integrated OpenShift Container Platform registry (OCR), OCP internal use, deployed on master nodes.\n",
      "\n",
      "Third Party Registries\n",
      "\n",
      "Red Hat Quay Registries\n",
      "\n",
      "Authentication enabled Red Hat registry\n",
      "\n",
      "In a scaled/high-availability (HA) OpenShift Container Platform registry cluster deployment:\n",
      "\n",
      "The storage technology must support RWX access mode.\n",
      "\n",
      "The storage technology must ensure read-after-write consistency.\n",
      "\n",
      "The preferred storage technology is object storage.\n",
      "\n",
      "Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.\n",
      "\n",
      "Object storage should be S3 or Swift compliant.\n",
      "\n",
      "For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.\n",
      "\n",
      "3.1.2.1 Key observation/challenges/requirements:\n",
      "\n",
      "NA\n",
      "\n",
      "\n",
      "\n",
      "3.1.2.2 Design decisions\n",
      "\n",
      "Registry: \n",
      "\n",
      "Registry (OCR) will be implemented in clusters and will run as a container.\n",
      "\n",
      "The registry stores container images and metadata. If you simply deploy the registry pod, it uses an ephemeral volume that is destroyed if the pod exits. Any images built or pushed into the registry would disappear.\n",
      "\n",
      "Hence, based on the available options at XXXs, OpenShift Container Platform registry storage will be provided from Netapp  (S3 compliant Object Storage) of size 1024 GiB(initial size, depending on the size of container images stored on it, it may need to be increased during day 2 operations).\n",
      "\n",
      "Netapp Object storage will be the backend for registry pods.\n",
      "\n",
      "3.1.3 Web Console\n",
      "\n",
      "The OpenShift Container Platform web console is a user interface accessible from a web browser. Developers can use the web console to visualize, browse, and manage the contents of projects. \n",
      "\n",
      "Web console will be deployed by default. System administrators may want to access the master apis via a command line tool called oc. Details are covered in the documentation.\n",
      "\n",
      "3.1.3.1 Design decisions\n",
      "\n",
      "Below decisions are applicable for all clusters.\n",
      "\n",
      "No use-cases were identified.  Default web console will be configured.\n",
      "\n",
      "3.2 Network Considerations\n",
      "\n",
      "3.2.1 OVN-Kubernetes & DNS\n",
      "\n",
      "The OpenShift Container Platform cluster uses a virtualized network for pod and service networks. The OVN-Kubernetes Container Network Interface (CNI) plug-in is a network provider for the default cluster network. OVN-Kubernetes is based on Open Virtual Network (OVN) and provides an overlay-based \n",
      "\n",
      "\n",
      "\n",
      "networking implementation. A cluster that uses the OVN-Kubernetes network provider also runs Open vSwitch (OVS) on each node. OVN configures OVS on each node to implement the declared network configuration.\n",
      "\n",
      "\n",
      "\n",
      "3.2.1.1 Design decisions\n",
      "\n",
      "XXX will use the default OVN-Kubernetes plugin.\n",
      "\n",
      "RHOCP Cluster will be based on dual stack (IPv4 & IPv6).\n",
      "\n",
      "The host network CIDR range for the OCP nodes will be provided by XXX.\n",
      "\n",
      "Cluster and Service subnet will be provided by XXX.\n",
      "\n",
      "XXX have confirmed that Egress IP may be used, depending on the application requirement.\n",
      "\n",
      "All OCP nodes will be reachable to each other. There is no microsegmentation at XXX Network.\n",
      "\n",
      "The cluster subnet and service subnet will be internal to OCP cluster however if applications hosted on this cluster need to connect with some external endpoint must not use IP from this cluster subnet and service subnet.\n",
      "\n",
      "3.2.2 Ingress Controllers (Routers)\n",
      "\n",
      "The Ingress Operator implements the ingress controller API and is the component responsible for enabling external access to OpenShift Container Platform cluster services. The Operator makes this possible by \n",
      "\n",
      "\n",
      "\n",
      "deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. \t\t\n",
      "\n",
      "OCP offers multiple ways in which HAProxy can be used via route objects. Please find the key features/configuration in documentation. \n",
      "\n",
      "\n",
      "\n",
      "3.2.2.1 Design decisions & Recommendations\n",
      "\n",
      "Below decisions are applicable for the cluster. \n",
      "\n",
      "3 router pods will be deployed on the master nodes.\n",
      "\n",
      "RHOCP router is expected to work with HTTP/HTTPS protocol only.\n",
      "\n",
      "By default; router will expose the application over port 443 for https traffic and port 80 for http.\n",
      "\n",
      "The OCP router will be responsible to route the traffic to the pods as per the workflow.\n",
      "\n",
      "3.2.2.2 Application flow\n",
      "\n",
      "VIP will be configured to pass the traffic to the router pods.\n",
      "\n",
      "Applications can utilize the same VIP and expose the application on intranet. Like https://sample-application.apps.devocp.example.local\n",
      "\n",
      "XXX application team may have different flow and that details will be documented under XXX approach document e.g. they might want to create their custom LB or flows to enable migration from OCP 3.11 to 4.14.\n",
      "\n",
      "3.2.3 Load Balancer\n",
      "\n",
      "\tXXX confirmed installation method would be UPI, below the load balancer requirement.\n",
      "\n",
      "Note: \n",
      "\n",
      "The load balancer must be configured to take a maximum of 30 seconds from the time the API server turns off the /readyz endpoint to the removal of the API server instance from the pool. Within the time frame after /readyz returns an error or becomes healthy, the endpoint must have been removed or added. Probing every 5 or 10 seconds, with two successful requests to become healthy and three to become unhealthy, are well-tested values.\n",
      "\n",
      "\n",
      "\n",
      "3.2.3.1 API load balancer \n",
      "\n",
      "\tXXX would configure F5 load balancer for all external API access as below. \n",
      "\n",
      "API endpoint Port VIP ssl mode Backend server Comments api.<TBF> 6443 passthrough bootstrap master 1 master 2 master 3 bootstrap is temporary and it has to be removed once the cluster is built.\n",
      "\n",
      "Note: \n",
      "\n",
      "Session persistence is not required for the API load balancer to function properly.\n",
      "\n",
      "3.2.3.2 API-INT load balancer\n",
      "\n",
      "XXX would configure F5 load balancer for all internal API access for the cluster below. \n",
      "\n",
      "API endpoint Port VIP ssl mode Backend server Comments api-int.<TBF> 22623 and 6443 passthrough bootstrap master 1 master 2 master 3 bootstrap is temporary and it has to be removed once the cluster is built.\n",
      "\n",
      "Note: \n",
      "\n",
      "Session persistence is not required for the API load balancer to function properly.\n",
      "\n",
      "3.2.3.3 *.APPS load balancer \n",
      "\n",
      "XXX would configure F5 load balancer for all application ingress access as below. \n",
      "\n",
      "API endpoint Port VIP ssl mode Backend server Remarks *.apps.<TBF> 443 and 80 passthrough master 1 master 2 master 3 As Router pods will be co-hosted on masters. If customer decides to move router modes from master to dedicate nodes, these IPs need to be updated on respective LB\n",
      "\n",
      "Note: \n",
      "\n",
      "If the true IP address of the client can be seen by the application Ingress load balancer, enabling source IP-based session persistence can improve performance for applications that use end-to-end TLS encryption.\n",
      "\n",
      "\n",
      "\n",
      "3.3 Prometheus Cluster Monitoring\n",
      "\n",
      "OpenShift Container Platform ships with a pre-configured and self-updating monitoring stack that is based on the Prometheus open source project and its wider ecosystem. It provides monitoring of cluster components and ships with a set of alerts to immediately notify the cluster administrator about any occurring problems and a set of OCP monitoring dashboards.\n",
      "\n",
      "You also have the option to enable monitoring for user-defined projects.\n",
      "\n",
      "A cluster administrator can configure the monitoring stack with the supported configurations. OpenShift Container Platform delivers monitoring best practices out of the box.\n",
      "\n",
      "A set of alerts are included by default that immediately notify administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. With the OpenShift Container Platform web console, you can view and manage metrics, alerts, and review monitoring dashboards.\n"
     ]
    }
   ],
   "source": [
    "top_searches = vector_store.similarity_search(query=\"Tell me about control plane?\", k=2)\n",
    "\n",
    "for search in top_searches:\n",
    "    print(search.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
